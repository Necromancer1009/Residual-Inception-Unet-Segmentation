{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.class_to_idx = {\n",
    "            'glioma': 0, \n",
    "            'meningioma': 1, \n",
    "            'notumor': 2, \n",
    "            'pituitary': 3\n",
    "        }\n",
    "        for class_name in os.listdir(root_dir):\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            if os.path.isdir(class_dir) and class_name in self.class_to_idx:\n",
    "                for img_name in os.listdir(class_dir):\n",
    "                    img_path = os.path.join(class_dir, img_name)\n",
    "                    self.images.append(img_path)\n",
    "                    self.labels.append(self.class_to_idx[class_name])\n",
    "                        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "    \n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(image_dir)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        mask_path = os.path.join(self.mask_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        mask = Image.open(mask_path).convert('L')  # Convert mask to grayscale\n",
    "        if self.transform:\n",
    "            image, mask = self.transform(image, mask)\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ResidualInceptionBlock(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels, stride=1):\n",
    "#         super(ResidualInceptionBlock, self).__init__()\n",
    "#         self.branch1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, padding=0)\n",
    "        \n",
    "#         self.branch2 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "#         )\n",
    "        \n",
    "#         self.branch3 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(out_channels, out_channels, kernel_size=5, stride=stride, padding=2)\n",
    "#         )\n",
    "        \n",
    "#         self.branch_pool = nn.Sequential(\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=stride, padding=1),\n",
    "#             nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0)\n",
    "#         )\n",
    "        \n",
    "#         self.bn = nn.BatchNorm2d(4 * out_channels)\n",
    "#         self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "#         # Residual connection\n",
    "#         self.shortcut = nn.Sequential()\n",
    "#         if stride != 1 or in_channels != 4 * out_channels:\n",
    "#             self.shortcut = nn.Sequential(\n",
    "#                 nn.Conv2d(in_channels, 4 * out_channels, kernel_size=1, stride=stride),\n",
    "#                 nn.BatchNorm2d(4 * out_channels)\n",
    "#             )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         branch1 = self.branch1(x)\n",
    "        \n",
    "#         branch2 = self.branch2(x)\n",
    "        \n",
    "#         branch3 = self.branch3(x)\n",
    "        \n",
    "#         branch4 = self.branch_pool(x)\n",
    "        \n",
    "#         outputs = torch.cat([branch1, branch2, branch3, branch4], 1)\n",
    "#         outputs = self.bn(outputs)\n",
    "        \n",
    "#         residual = self.shortcut(x)\n",
    "#         outputs += residual\n",
    "#         outputs = self.relu(outputs)\n",
    "#         return outputs\n",
    "\n",
    "# class DetectionModel(nn.Module):\n",
    "#     def __init__(self, num_classes=2):\n",
    "#         super(DetectionModel, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "#         self.bn1   = nn.BatchNorm2d(64)\n",
    "#         self.relu  = nn.ReLU(inplace=True)\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "#         self.layer1 = ResidualInceptionBlock(64, 64)\n",
    "#         self.layer2 = ResidualInceptionBlock(256, 128, stride=2)\n",
    "#         self.layer3 = ResidualInceptionBlock(512, 256, stride=2)\n",
    "#         self.layer4 = ResidualInceptionBlock(1024, 512, stride=2)\n",
    "        \n",
    "#         self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "#         self.fc      = nn.Linear(2048, num_classes)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.relu(self.bn1(self.conv1(x)))  # [B, 64, H/2, W/2]\n",
    "#         x = self.maxpool(x)  # [B, 64, H/4, W/4]\n",
    "        \n",
    "#         x = self.layer1(x)   # [B, 256, H/4, W/4]\n",
    "#         x = self.layer2(x)   # [B, 512, H/8, W/8]\n",
    "#         x = self.layer3(x)   # [B, 1024, H/16, W/16]\n",
    "#         x = self.layer4(x)   # [B, 2048, H/32, W/32]\n",
    "        \n",
    "#         x = self.avgpool(x)  # [B, 2048, 1, 1]\n",
    "#         x = x.view(x.size(0), -1)  # [B, 2048]\n",
    "#         x = self.fc(x)       # [B, num_classes]\n",
    "#         return x\n",
    "\n",
    "# # Instantiate the model\n",
    "# def create_detection_model(num_classes=2):\n",
    "#     model = DetectionModel(num_classes=num_classes)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualInceptionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualInceptionBlock, self).__init__()\n",
    "        # Branch 1\n",
    "        self.branch1 = nn.Conv2d(in_channels, out_channels // 4, kernel_size=1, stride=stride)\n",
    "\n",
    "        # Branch 2\n",
    "        self.branch2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels // 4, kernel_size=1, stride=stride),\n",
    "            nn.BatchNorm2d(out_channels // 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels // 4, out_channels // 4, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "        # Branch 3\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels // 4, kernel_size=1, stride=stride),\n",
    "            nn.BatchNorm2d(out_channels // 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels // 4, out_channels // 4, kernel_size=5, stride=1, padding=2)\n",
    "        )\n",
    "\n",
    "        # Branch 4 (Pooling)\n",
    "        self.branch_pool = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=stride, padding=1),\n",
    "            nn.Conv2d(in_channels, out_channels // 4, kernel_size=1, stride=1, padding=0)\n",
    "        )\n",
    "\n",
    "        # Batch Norm and Activation\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Shortcut Connection\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels or stride != 1:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1 = self.branch1(x)\n",
    "        branch2 = self.branch2(x)\n",
    "        branch3 = self.branch3(x)\n",
    "        branch4 = self.branch_pool(x)\n",
    "        outputs = torch.cat([branch1, branch2, branch3, branch4], 1)\n",
    "        outputs = self.bn(outputs)\n",
    "        residual = self.shortcut(x)\n",
    "        outputs += residual\n",
    "        outputs = self.relu(outputs)\n",
    "        return outputs\n",
    "    \n",
    "def make_layer(block, in_channels, out_channels, num_blocks, stride=1):\n",
    "    layers = []\n",
    "    layers.append(block(in_channels, out_channels, stride=stride))\n",
    "    in_channels = out_channels  # Update in_channels for subsequent blocks\n",
    "    for _ in range(1, num_blocks):\n",
    "        layers.append(block(in_channels, out_channels, stride=1))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "# class DeeperDetectionModel(nn.Module):\n",
    "#     def __init__(self, num_classes=2):\n",
    "#         super(DeeperDetectionModel, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "#         self.bn1   = nn.BatchNorm2d(64)\n",
    "#         self.relu  = nn.ReLU(inplace=True)\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "#         # Define the number of blocks in each layer\n",
    "#         self.layer1 = make_layer(ResidualInceptionBlock, 64, 128, num_blocks=3, stride=1)\n",
    "#         self.layer1_5 = make_layer(ResidualInceptionBlock, 128, 256, num_blocks=3, stride=1)\n",
    "#         self.layer2 = make_layer(ResidualInceptionBlock, 256, 512, num_blocks=4, stride=2)\n",
    "#         # self.layer2_5 = make_layer(ResidualInceptionBlock, 512, 512, num_blocks=4, stride=1)\n",
    "#         self.layer2_5 = make_layer(ResidualInceptionBlock, 512, 512, num_blocks=6, stride=1)\n",
    "#         self.layer3 = make_layer(ResidualInceptionBlock, 512, 1024, num_blocks=6, stride=2)\n",
    "#         self.layer4 = make_layer(ResidualInceptionBlock, 1024, 2048, num_blocks=3, stride=2)\n",
    "\n",
    "#         self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "#         self.fc      = nn.Linear(2048, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.relu(self.bn1(self.conv1(x)))  # [B, 64, H/2, W/2]\n",
    "#         x = self.maxpool(x)                     # [B, 64, H/4, W/4]\n",
    "\n",
    "#         x = self.layer1(x)                      # [B, 256, H/4, W/4]\n",
    "#         x = self.layer1_5(x)                    # [B, 256, H/4, W/4]\n",
    "#         x = self.layer2(x)                      # [B, 512, H/8, W/8]\n",
    "#         x = self.layer2_5(x)                    # [B, 512, H/8, W/8]\n",
    "#         x = self.layer3(x)                      # [B, 1024, H/16, W/16]\n",
    "#         x = self.layer4(x)                      # [B, 2048, H/32, W/32]\n",
    "\n",
    "#         x = self.avgpool(x)                     # [B, 2048, 1, 1]\n",
    "#         x = x.view(x.size(0), -1)               # [B, 2048]\n",
    "#         x = self.fc(x)                          # [B, num_classes]\n",
    "#         return x\n",
    "    \n",
    "class DeeperDetectionModel(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(DeeperDetectionModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1   = nn.BatchNorm2d(64)\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Define the number of blocks in each layer\n",
    "        self.layer1 = make_layer(ResidualInceptionBlock, 64, 256, num_blocks=3, stride=1)\n",
    "        self.layer2 = make_layer(ResidualInceptionBlock, 256, 512, num_blocks=4, stride=2)\n",
    "        self.layer3 = make_layer(ResidualInceptionBlock, 512, 1024, num_blocks=6, stride=2)\n",
    "        self.layer4 = make_layer(ResidualInceptionBlock, 1024, 2048, num_blocks=3, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc      = nn.Linear(2048, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))  # [B, 64, H/2, W/2]\n",
    "        x = self.maxpool(x)                     # [B, 64, H/4, W/4]\n",
    "\n",
    "        x = self.layer1(x)                      # [B, 256, H/4, W/4]\n",
    "        x = self.layer2(x)                      # [B, 512, H/8, W/8]\n",
    "        x = self.layer3(x)                      # [B, 1024, H/16, W/16]\n",
    "        x = self.layer4(x)                      # [B, 2048, H/32, W/32]\n",
    "\n",
    "        x = self.avgpool(x)                     # [B, 2048, 1, 1]\n",
    "        x = x.view(x.size(0), -1)               # [B, 2048]\n",
    "        x = self.fc(x)                          # [B, num_classes]\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "def create_detection_model(num_classes=2):\n",
    "    model = DeeperDetectionModel(num_classes=num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualInceptionBlock_segmentation(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualInceptionBlock_segmentation, self).__init__()\n",
    "        self.branch1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, padding=0)\n",
    "        \n",
    "        self.branch2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        )\n",
    "        \n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=5, stride=stride, padding=2)\n",
    "        )\n",
    "        \n",
    "        self.branch_pool = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=stride, padding=1),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0)\n",
    "        )\n",
    "        \n",
    "        self.bn = nn.BatchNorm2d(4 * out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Residual connection\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != 4 * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, 4 * out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(4 * out_channels)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        branch1 = self.branch1(x)\n",
    "        \n",
    "        branch2 = self.branch2(x)\n",
    "        \n",
    "        branch3 = self.branch3(x)\n",
    "        \n",
    "        branch4 = self.branch_pool(x)\n",
    "        \n",
    "        outputs = torch.cat([branch1, branch2, branch3, branch4], 1)\n",
    "        outputs = self.bn(outputs)\n",
    "        \n",
    "        residual = self.shortcut(x)\n",
    "        outputs += residual\n",
    "        outputs = self.relu(outputs)\n",
    "        return outputs\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, skip_channels, out_channels):\n",
    "        super(UpBlock, self).__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.rib = ResidualInceptionBlock_segmentation(out_channels + skip_channels, out_channels, stride=1)\n",
    "        \n",
    "    def forward(self, x, skip_connection):\n",
    "        x = self.up(x)  # Upsample\n",
    "        x = torch.cat([x, skip_connection], dim=1)  # Concatenate with skip connection\n",
    "        x = self.rib(x)  # Pass through Residual Inception Block\n",
    "        return x\n",
    "\n",
    "class SegmentationModel(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1):\n",
    "        super(SegmentationModel, self).__init__()\n",
    "        self.enc1 = ResidualInceptionBlock_segmentation(in_channels, 32)  # Output channels: 128\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.enc2 = ResidualInceptionBlock_segmentation(128, 64)  # Output channels: 256\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.enc3 = ResidualInceptionBlock_segmentation(256, 128)  # Output channels: 512\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.enc4 = ResidualInceptionBlock_segmentation(512, 256)  # Output channels: 1024\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.bottleneck = ResidualInceptionBlock_segmentation(1024, 512)  # Output channels: 2048\n",
    "        \n",
    "        # Decoder with corrected channel dimensions\n",
    "        self.up4 = UpBlock(in_channels=2048, skip_channels=1024, out_channels=256)\n",
    "        self.up3 = UpBlock(in_channels=1024, skip_channels=512, out_channels=128)\n",
    "        self.up2 = UpBlock(in_channels=512, skip_channels=256, out_channels=64)\n",
    "        self.up1 = UpBlock(in_channels=256, skip_channels=128, out_channels=32)\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(128, out_channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1 = self.enc1(x)  # [B, 128, H, W]\n",
    "        enc2 = self.enc2(self.pool1(enc1))  # [B, 256, H/2, W/2]\n",
    "        enc3 = self.enc3(self.pool2(enc2))  # [B, 512, H/4, W/4]\n",
    "        enc4 = self.enc4(self.pool3(enc3))  # [B, 1024, H/8, W/8]\n",
    "        \n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(self.pool4(enc4))  # [B, 2048, H/16, W/16]\n",
    "        \n",
    "        # Decoder\n",
    "        dec4 = self.up4(bottleneck, enc4)  # [B, 1024, H/8, W/8]\n",
    "        dec3 = self.up3(dec4, enc3)        # [B, 512, H/4, W/4]\n",
    "        dec2 = self.up2(dec3, enc2)        # [B, 256, H/2, W/2]\n",
    "        dec1 = self.up1(dec2, enc1)        # [B, 128, H, W]\n",
    "        \n",
    "        out = self.final_conv(dec1)        # [B, out_channels, H, W]\n",
    "        return out\n",
    "\n",
    "# Instantiate the model\n",
    "def create_segmentation_model(in_channels=3, out_channels=1):\n",
    "    model = SegmentationModel(in_channels=in_channels, out_channels=out_channels)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class SegmentationTransform:\n",
    "    def __init__(self):\n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                 [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "            \n",
    "    def __call__(self, image, mask):\n",
    "        image = self.image_transform(image)\n",
    "        mask = mask.resize((224, 224), resample=Image.NEAREST)\n",
    "        mask = np.array(mask, dtype=np.float32)\n",
    "        mask = np.expand_dims(mask, axis=0)  # Add channel dimension\n",
    "        mask = torch.from_numpy(mask)\n",
    "        mask = mask / 255.0  # Scale mask to [0, 1] range if needed\n",
    "        return image, mask\n",
    "\n",
    "train_dataset = ClassificationDataset(root_dir='./Dataset_Arghadip/Train', transform=classification_transform)\n",
    "val_dataset = ClassificationDataset(root_dir='./Dataset_Arghadip/Validation', transform=classification_transform)\n",
    "test_dataset = ClassificationDataset(root_dir='./Dataset_Arghadip/Test', transform=classification_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "segmentation_image_dir = './Segmentaion_dataset/images'\n",
    "segmentation_mask_dir = './Segmentaion_dataset/masks'\n",
    "\n",
    "segmentation_transform = SegmentationTransform()\n",
    "segmentation_dataset = SegmentationDataset(image_dir=segmentation_image_dir,\n",
    "                                           mask_dir=segmentation_mask_dir,\n",
    "                                           transform=segmentation_transform)\n",
    "\n",
    "segmentation_loader = DataLoader(segmentation_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification_model = models.resnet18(pretrained=True)\n",
    "# num_ftrs = classification_model.fc.in_features\n",
    "# classification_model.fc = nn.Linear(num_ftrs, 4)  # 4 classes\n",
    "# classification_model = classification_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segmentation_model = models.segmentation.fcn_resnet50(pretrained=False, num_classes=1)\n",
    "# segmentation_model = segmentation_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1 = MICRO_SEM(3)\n",
    "# model2 = MESO_SEM(3)\n",
    "# model3 = MACRO_SEM(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchinfo import summary\n",
    "# summary_ = summary(model3, input_size = (32, 3, 224, 224))\n",
    "# summary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segmentation_model = create_segmentation_model(3, 1)\n",
    "# segmentation_model = segmentation_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = create_detection_model(num_classes = 4)\n",
    "classification_model = classification_model.to(device)\n",
    "segmentation_model = create_segmentation_model(3, 1)\n",
    "segmentation_model = segmentation_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchview import draw_graph\n",
    "import graphviz as gv\n",
    "gv.set_jupyter_format('svg')\n",
    "\n",
    "classification_model_graph = draw_graph(classification_model, input_size = (8, 3, 256, 256), graph_name = 'classification_model', expand_nested = True, save_graph = True, directory = \"./Architecture\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_model_graph = draw_graph(segmentation_model, input_size = (8, 3, 256, 256), graph_name = 'segmentation_model', expand_nested = True, save_graph = True, directory = \"./Architecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Architecture\\\\segmentation_model.gv.pdf'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gv.render(engine = 'dot', format = 'svg', filepath = './Architecture/classification_model.gv')\n",
    "gv.render(engine = 'dot', format = 'pdf', filepath = './Architecture/classification_model.gv')\n",
    "\n",
    "gv.render(engine = 'dot', format = 'svg', filepath = './Architecture/segmentation_model.gv')\n",
    "gv.render(engine = 'dot', format = 'pdf', filepath = './Architecture/segmentation_model.gv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "summary_ = summary(classification_model, input_size = (16, 3, 256, 256))\n",
    "summary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion_segmentation = nn.BCEWithLogitsLoss()\n",
    "# optimizer_segmentation = optim.Adam(segmentation_model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_classification = nn.CrossEntropyLoss()\n",
    "optimizer_classification = optim.Adam(classification_model.parameters(), lr=1e-4)\n",
    "# optimizer_classification = optim.AdamW(classification_model.parameters(), lr = 0.001, weight_decay = 0.01, amsgrad = True)\n",
    "\n",
    "criterion_segmentation = nn.BCEWithLogitsLoss()\n",
    "optimizer_segmentation = optim.Adam(segmentation_model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 10\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     running_corrects = 0\n",
    "    \n",
    "#     for inputs, labels in train_loader:\n",
    "#         inputs = inputs.to(device)\n",
    "#         labels = labels.to(device)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         outputs = model(inputs)\n",
    "#         _, preds = torch.max(outputs, 1)\n",
    "#         loss = criterion(outputs, labels)\n",
    "        \n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         running_loss += loss.item() * inputs.size(0)\n",
    "#         running_corrects += torch.sum(preds == labels.data)\n",
    "    \n",
    "#     epoch_loss = running_loss / len(train_loader)\n",
    "#     epoch_acc = running_corrects.double() / len(train_loader)\n",
    "    \n",
    "#     print('Epoch {}/{}: Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "#         epoch+1, num_epochs, epoch_loss, epoch_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow.start_run()\n",
    "# mlflow.log_param(\"batch_size\", 16)\n",
    "# mlflow.log_param(\"learning_rate\", 0.001)\n",
    "# mlflow.log_param(\"num_epochs\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "num_epochs = 100\n",
    "total_len = len(train_loader) + len(val_loader)\n",
    "\n",
    "train_losses = np.zeros(num_epochs)\n",
    "test_losses = np.zeros(num_epochs)\n",
    "train_acc = np.zeros(num_epochs)\n",
    "test_acc = np.zeros(num_epochs)\n",
    "\n",
    "best_test_acc = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_str = str(epoch + 1).rjust(len(str(num_epochs)), \" \")\n",
    "    with tqdm(total = total_len, desc = f\"Epoch [ {epoch_str}/{num_epochs} ] : \") as pbar:\n",
    "        \n",
    "        classification_model.train()\n",
    "        n_correct = 0\n",
    "        n_total = 0\n",
    "        train_loss = []\n",
    "        \n",
    "        for j, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer_classification.zero_grad()\n",
    "            \n",
    "            outputs = classification_model(inputs)\n",
    "            loss = criterion_classification(outputs, targets)\n",
    "            train_loss.append(loss.item())\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer_classification.step()\n",
    "            \n",
    "            _, prediction = torch.max(outputs, 1)\n",
    "            \n",
    "            n_correct += (prediction == targets).sum().item()\n",
    "            n_total += targets.shape[0]\n",
    "            \n",
    "            # mlflow.log_metric(\"train_loss\", loss.item(), step=epoch * len(train_loader) + j)\n",
    "            # mlflow.log_metric(\"train_accuracy\", n_correct / n_total, step=epoch * len(train_loader) + j)\n",
    "            \n",
    "            pbar.update(1)\n",
    "        \n",
    "        # before_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        # scheduler.step()\n",
    "        # after_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        \n",
    "        train_loss = np.mean(train_loss)\n",
    "        train_acc_real = n_correct/n_total\n",
    "        train_losses[epoch] = train_loss\n",
    "        train_acc[epoch] = train_acc_real\n",
    "                \n",
    "        classification_model.eval()\n",
    "        n_correct = 0\n",
    "        n_total = 0\n",
    "        test_loss = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, (inputs, targets) in enumerate(val_loader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                \n",
    "                outputs = classification_model(inputs)\n",
    "                \n",
    "                loss = criterion_classification(outputs, targets)\n",
    "                test_loss.append(loss.item())\n",
    "                \n",
    "                _, prediction = torch.max(outputs, 1)\n",
    "                \n",
    "                n_correct += (prediction == targets).sum().item()\n",
    "                n_total += targets.shape[0]\n",
    "                \n",
    "                pbar.update(1)\n",
    "                \n",
    "            mlflow.log_metric(\"valid_loss\", loss.item(), step=epoch * len(val_loader) + i)\n",
    "            mlflow.log_metric(\"valid_accuracy\", n_correct / n_total, step=epoch * len(val_loader) + i)\n",
    "            \n",
    "        test_loss = np.mean(test_loss)\n",
    "        test_acc_real = n_correct/n_total\n",
    "        test_losses[epoch] = test_loss\n",
    "        test_acc[epoch] = test_acc_real\n",
    "        \n",
    "        pbar.set_description(f\"Epoch [ {epoch_str}/{num_epochs} ] \")\n",
    "        pbar.set_postfix({'Train acc' : f'{train_acc_real:.3f}',\n",
    "                          'Train loss' : f'{train_loss:.3f}',\n",
    "                          'Test acc' : f'{test_acc_real:.3f}',\n",
    "                          'Test loss': f'{test_loss:.3f}'\n",
    "                        #   'lr_before' : f'{before_lr}',\n",
    "                        #   'lr_after' : f'{after_lr}'\n",
    "                         })\n",
    "        \n",
    "        # mlflow.log_metric(\"train_loss_epoch\", train_loss, step=epoch)\n",
    "        # mlflow.log_metric(\"train_accuracy_epoch\", train_acc_real, step=epoch)\n",
    "        # mlflow.log_metric(\"valid_loss_epoch\", test_loss, step=epoch)\n",
    "        # mlflow.log_metric(\"valid_accuracy_epoch\", test_acc_real, step=epoch)\n",
    "            \n",
    "        if test_acc_real >= best_test_acc:\n",
    "            best_test_acc = test_acc_real\n",
    "            test_str = f\"{int(test_acc_real * 1000)/10}\"\n",
    "            train_str = f\"{int(train_acc_real * 1000)/10}\"\n",
    "            torch.save(classification_model.state_dict(), f\"./Models/Retrain/classification_model_train_{epoch}_{train_str}_test_{test_str}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow.pytorch.log_model(classification_model, \"model\")\n",
    "# mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ClassificationDataset(root_dir='./New_datasets/archive_2/Testing', transform=classification_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trained = create_detection_model(num_classes = 4)\n",
    "model_trained.load_state_dict(torch.load(\"./Models/classification_model_train_41_99.8_test_99.0.pth\", weights_only=True))\n",
    "model_trained.to(device)\n",
    "model_trained.eval()\n",
    "\n",
    "val_running_corrects = 0\n",
    "\n",
    "n_correct = 0\n",
    "n_total = 0\n",
    "\n",
    "n_correct_ = 0\n",
    "n_total_ = 0\n",
    "\n",
    "y_prob = []\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in val_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "                \n",
    "        outputs = model_trained(inputs)\n",
    "        probs_ = torch.softmax(outputs, dim = 1)\n",
    "        _, prediction_ = torch.max(outputs, 1)\n",
    "        prediction__ = (probs_ >= 0.5) * 1\n",
    "        \n",
    "        probs_cpu = probs_.cpu().detach().numpy()\n",
    "        preds_cpu = prediction_.cpu().detach().numpy()\n",
    "        targets_cpu = targets.cpu().detach().numpy()\n",
    "        \n",
    "        \n",
    "        for element in probs_cpu:\n",
    "            y_prob.append(element)\n",
    "    \n",
    "        for element in preds_cpu:\n",
    "            y_pred.append(element)\n",
    "        \n",
    "        for element in targets_cpu:\n",
    "            y_true.append(element)\n",
    "                \n",
    "        n_correct += (prediction_ == targets).sum().item()\n",
    "        n_total += targets.shape[0]\n",
    "\n",
    "test_acc_real = n_correct/n_total\n",
    "print('Test Accuracy: {:.4f}'.format(test_acc_real))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob_np = np.array(y_prob)\n",
    "y_pred_np = np.array(y_pred).reshape(-1, 1)\n",
    "y_true_np = np.array(y_true).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "y_true_bin = label_binarize(y_true_np, classes=[0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, roc_auc_score, precision_score, confusion_matrix, f1_score, accuracy_score, recall_score, roc_curve, auc\n",
    "\n",
    "cm = confusion_matrix(y_true_np, y_pred_np)\n",
    "r2 = r2_score(y_true_np, y_pred_np)\n",
    "ras = roc_auc_score(y_true_np, y_prob_np, multi_class = 'ovr')\n",
    "ps = precision_score(y_true_np, y_pred_np, average = 'weighted')\n",
    "f1 = f1_score(y_true_np, y_pred_np, average = 'weighted')\n",
    "acc = accuracy_score(y_true_np, y_pred_np)\n",
    "rs = recall_score(y_true_np, y_pred_np, average = 'weighted')\n",
    "# fpr, tpr, _ = roc_curve(y_true_np, y_prob_np)\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "n_classes = 4\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_prob_np[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix : \")\n",
    "print(cm, \"\\n\")\n",
    "print(\"R2 Score : \", r2)\n",
    "print(\"ROC AUC Score : \", ras)\n",
    "print(\"Precision Score : \", ps)\n",
    "print(\"F1 Score : \", f1)\n",
    "print(\"Accuracy : \", acc)\n",
    "print(\"Recall Score : \", rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin.ravel(), y_prob_np.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Plot ROC curve for each class with improved aesthetics\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Use a color map to make it more visually appealing\n",
    "colors = ['#4379F2', '#604CC3', '#6EC207', '#FF6600']\n",
    "\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i], color=colors[i], lw=2, \n",
    "             label=f'Class {i} (AUC = {roc_auc[i]:0.2f})')\n",
    "\n",
    "# Plot the micro-average ROC curve\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"], color='#1A3636', linestyle='dashed', linewidth=2,\n",
    "         label=f'Micro-average ROC curve (AUC = {roc_auc[\"micro\"]:0.2f})')\n",
    "\n",
    "# Diagonal line representing the random classifier\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "\n",
    "# Customize the plot aesthetics\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=14)\n",
    "plt.ylabel('True Positive Rate', fontsize=14)\n",
    "plt.title('Multiclass ROC Curve', fontsize=18, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\", fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "# If you want the macro-average AUC score across all classes\n",
    "roc_auc_macro = roc_auc_score(y_true_bin, y_prob_np, average=\"macro\", multi_class=\"ovr\")\n",
    "print(f\"Macro-average AUC: {roc_auc_macro:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin.ravel(), y_prob_np.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Create the plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Colors for each class\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "# Add ROC curves for each class\n",
    "for i in range(n_classes):\n",
    "    fig.add_trace(go.Scatter(x=fpr[i], y=tpr[i], mode='lines', \n",
    "                             name=f'Class {i} (AUC = {roc_auc[i]:0.2f})',\n",
    "                             line=dict(color=colors[i], width=3)))\n",
    "\n",
    "# Add micro-average ROC curve\n",
    "fig.add_trace(go.Scatter(x=fpr[\"micro\"], y=tpr[\"micro\"], mode='lines', \n",
    "                         name=f'Micro-average ROC (AUC = {roc_auc[\"micro\"]:0.2f})',\n",
    "                         line=dict(color='deeppink', width=4, dash='dot')))\n",
    "\n",
    "# Add diagonal line for random classifier\n",
    "fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines', \n",
    "                         name='Random Classifier', line=dict(color='black', dash='dash')))\n",
    "\n",
    "# Update layout for better aesthetics\n",
    "fig.update_layout(\n",
    "    title=\"Multiclass ROC Curve\",\n",
    "    xaxis_title=\"False Positive Rate\",\n",
    "    yaxis_title=\"True Positive Rate\",\n",
    "    font=dict(size=14),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    legend=dict(x=0.8, y=0.2),\n",
    "    plot_bgcolor='white'\n",
    ")\n",
    "\n",
    "# Add gridlines for clarity\n",
    "fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='LightGray')\n",
    "fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='LightGray')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "# If you want the macro-average AUC score across all classes\n",
    "roc_auc_macro = roc_auc_score(y_true_bin, y_prob_np, average=\"macro\", multi_class=\"ovr\")\n",
    "print(f\"Macro-average AUC: {roc_auc_macro:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model.eval()\n",
    "val_running_corrects = 0\n",
    "\n",
    "n_correct = 0\n",
    "n_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "                \n",
    "        outputs = classification_model(inputs)\n",
    "\n",
    "        _, prediction = torch.max(outputs, 1)\n",
    "                \n",
    "        n_correct += (prediction == targets).sum().item()\n",
    "        n_total += targets.shape[0]\n",
    "\n",
    "test_acc_real = n_correct/n_total\n",
    "print('Validation Accuracy: {:.4f}'.format(test_acc_real))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m outputs \u001b[38;5;241m=\u001b[39m segmentation_model(inputs)\n\u001b[0;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion_segmentation(outputs, masks\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m---> 21\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m optimizer_segmentation\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     23\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\sayan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sayan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sayan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:817\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    815\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    816\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    818\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    819\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    820\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import tqdm as tqdm_sim\n",
    "\n",
    "best_epoch_loss = 1\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_str = str(epoch + 1).rjust(len(str(num_epochs)), \" \")\n",
    "    with tqdm(total = len(segmentation_loader), desc = f\"Epoch [ {epoch_str}/{num_epochs} ] : \") as pbar:\n",
    "        segmentation_model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, masks in segmentation_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            # print(inputs.shape, masks.shape, outputs.shape)\n",
    "            \n",
    "            optimizer_segmentation.zero_grad()\n",
    "            outputs = segmentation_model(inputs)\n",
    "            loss = criterion_segmentation(outputs, masks.float())\n",
    "            loss.backward()\n",
    "            optimizer_segmentation.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            pbar.update(1)\n",
    "            \n",
    "        epoch_loss = running_loss / len(segmentation_dataset)\n",
    "        pbar.set_description(f\"Epoch [ {epoch_str}/{num_epochs} ] \")\n",
    "        pbar.set_postfix({'Train loss' : f'{epoch_loss:.3f}'})\n",
    "        \n",
    "        if epoch_loss <= best_epoch_loss:\n",
    "            best_epoch_loss = epoch_loss\n",
    "            train_str = f\"{best_epoch_loss:.3f}\"\n",
    "            torch.save(segmentation_model.state_dict(), f\"./Models/segmentation_model_train_{epoch}_{train_str}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = create_detection_model(num_classes = 4)\n",
    "classification_model.load_state_dict(torch.load(\"./Models/classification_model_train_92_100.0_test_99.2.pth\", weights_only=True))\n",
    "classification_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "classification_model.eval()\n",
    "segmentation_model.eval()\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "classification_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "segmentation_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_image_path = './Dataset_Arghadip/Test/glioma/glioma (1).jpg'\n",
    "original_image = Image.open(test_image_path).convert('RGB')\n",
    "\n",
    "input_image_classification = classification_transform(original_image)\n",
    "input_image_classification = input_image_classification.unsqueeze(0).to(device)  # Shape: [1, 3, 224, 224]\n",
    "\n",
    "input_image_segmentation = segmentation_transform(original_image)\n",
    "input_image_segmentation = input_image_segmentation.unsqueeze(0).to(device)  # Shape: [1, 3, 224, 224]\n",
    "\n",
    "classification_output = classification_model(input_image_classification)\n",
    "_, predicted_class_idx = torch.max(classification_output, 1)\n",
    "predicted_class_idx = predicted_class_idx.item()\n",
    "\n",
    "idx_to_class = {0: 'glioma', 1: 'meningioma', 2: 'pituitary', 3: 'notumor'}\n",
    "predicted_class_name = idx_to_class[predicted_class_idx]\n",
    "\n",
    "segmentation_output = segmentation_model(input_image_segmentation)  # Output shape: [1, 1, 224, 224]\n",
    "\n",
    "segmentation_output = segmentation_output.squeeze(0).cpu()  # Shape: [1, 224, 224]\n",
    "segmentation_output = torch.sigmoid(segmentation_output)  # Values between 0 and 1\n",
    "threshold = 0.5\n",
    "mask = (segmentation_output > threshold).float()\n",
    "mask = mask.squeeze().numpy()  # Shape: [224, 224]\n",
    "\n",
    "mask = Image.fromarray((mask * 255).astype(np.uint8))\n",
    "mask = mask.resize(original_image.size, resample=Image.NEAREST)\n",
    "\n",
    "overlay = original_image.copy()\n",
    "overlay.putalpha(mask)\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "axs[0].imshow(original_image)\n",
    "axs[0].set_title('Original Image')\n",
    "axs[0].axis('off')\n",
    "\n",
    "axs[1].imshow(mask, cmap='gray')\n",
    "axs[1].set_title('Segmentation Mask')\n",
    "axs[1].axis('off')\n",
    "\n",
    "axs[2].imshow(overlay)\n",
    "axs[2].set_title('Overlay')\n",
    "axs[2].axis('off')\n",
    "\n",
    "axs[3].text(0.5, 0.5, f'Predicted Class:\\n{predicted_class_name}', fontsize=18, ha='center')\n",
    "axs[3].set_axis_off()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
