{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.class_to_idx = {\n",
    "            'glioma': 0,\n",
    "            'meningioma': 1,\n",
    "            'pituitary': 2,\n",
    "            'notumor': 3\n",
    "        }\n",
    "        for class_name in os.listdir(root_dir):\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            if os.path.isdir(class_dir) and class_name in self.class_to_idx:\n",
    "                for img_name in os.listdir(class_dir):\n",
    "                    img_path = os.path.join(class_dir, img_name)\n",
    "                    self.images.append(img_path)\n",
    "                    self.labels.append(self.class_to_idx[class_name])\n",
    "                        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "    \n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(image_dir)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        mask_path = os.path.join(self.mask_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        mask = Image.open(mask_path).convert('L')  # Convert mask to grayscale\n",
    "        if self.transform:\n",
    "            image, mask = self.transform(image, mask)\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MICRO_SEM(nn.Module):\n",
    "    def __init__(self, num_in):\n",
    "        super(MICRO_SEM, self).__init__()\n",
    "        self.conv_1 = nn.Conv2d(in_channels = num_in, out_channels = 32, kernel_size = (3, 3), padding = 'same')\n",
    "        self.relu = nn.ReLU()\n",
    "        self.batch_norm_1 = nn.BatchNorm2d(32)\n",
    "        self.pool_1 = nn.MaxPool2d(kernel_size = (3, 3))\n",
    "        \n",
    "        self.conv_2 = nn.Conv2d(in_channels = 32, out_channels = 48, kernel_size = (3, 3), padding = 'same')\n",
    "        self.batch_norm_2 = nn.BatchNorm2d(48)\n",
    "        self.pool_2 = nn.MaxPool2d(kernel_size = (3, 3))\n",
    "        \n",
    "        self.conv_3 = nn.Conv2d(in_channels = 48, out_channels = 96, kernel_size = (3, 3), padding = 'same')\n",
    "        self.batch_norm_3 = nn.BatchNorm2d(96)\n",
    "        self.pool_3 = nn.MaxPool2d(kernel_size = (3, 3))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batch_norm_1(x)\n",
    "        x = self.pool_1(x)\n",
    "        \n",
    "        x = self.conv_2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batch_norm_2(x)\n",
    "        x = self.pool_2(x)\n",
    "        \n",
    "        x = self.conv_3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batch_norm_3(x)\n",
    "        x = self.pool_3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class MESO_SEM(nn.Module):\n",
    "    def __init__(self, num_in):\n",
    "        super(MESO_SEM, self).__init__()\n",
    "        \n",
    "        self.conv_1_a = nn.Conv2d(in_channels = num_in, out_channels = 32, kernel_size = (3, 3), padding = 'same')\n",
    "        self.conv_1_b = nn.Conv2d(in_channels = 32, out_channels = 48, kernel_size = (3, 3), padding = 'same')\n",
    "        self.batch_norm_1 = nn.BatchNorm2d(48)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool_1 = nn.MaxPool2d(3, 3)\n",
    "        \n",
    "        self.conv_2_a = nn.Conv2d(in_channels = 48, out_channels = 64, kernel_size = (3, 3), padding = 'same')\n",
    "        self.conv_2_b = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = (3, 3), padding = 'same')\n",
    "        self.batch_norm_2 = nn.BatchNorm2d(64)\n",
    "        self.pool_2 = nn.MaxPool2d(3, 3)\n",
    "        \n",
    "        self.resize = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 48, out_channels = 64, kernel_size = (3, 3), stride = 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64)\n",
    "            )\n",
    "        \n",
    "        self.conv_3_a = nn.Conv2d(in_channels = 64, out_channels = 96, kernel_size = (3, 3), padding = 'same')\n",
    "        self.conv_3_b = nn.Conv2d(in_channels = 96, out_channels = 96, kernel_size = (3, 3), padding = 'same')\n",
    "        self.batch_norm_3 = nn.BatchNorm2d(96)\n",
    "        self.pool_3 = nn.MaxPool2d(3, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_1_a(x)\n",
    "        x = self.relu(x)      \n",
    "        x = self.conv_1_b(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batch_norm_1(x)\n",
    "        x = self.pool_1(x)\n",
    "        \n",
    "        x_resize = self.resize(x)\n",
    "        \n",
    "        x = self.conv_2_a(x)\n",
    "        x = self.relu(x)      \n",
    "        x = self.conv_2_b(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batch_norm_2(x)\n",
    "        x = self.pool_2(x)\n",
    "        \n",
    "        x = torch.add(x, x_resize)\n",
    "        \n",
    "        x = self.conv_3_a(x)\n",
    "        x = self.relu(x)      \n",
    "        x = self.conv_3_b(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batch_norm_3(x)\n",
    "        x = self.pool_3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class MACRO_SEM(nn.Module):\n",
    "    def __init__(self, num_in):\n",
    "        super(MACRO_SEM, self).__init__()\n",
    "        \n",
    "        self.conv_1_a = nn.Conv2d(in_channels = num_in, out_channels = 32, kernel_size = (5, 5), padding = 'same')\n",
    "        self.conv_1_b = nn.Conv2d(in_channels = 32, out_channels = 48, kernel_size = (5, 5), padding = 'same')\n",
    "        self.batch_norm_1 = nn.BatchNorm2d(48)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool_1 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.resize_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 48, out_channels = 64, kernel_size = (2, 2), stride = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64)\n",
    "            )\n",
    "        \n",
    "        self.conv_2_a = nn.Conv2d(in_channels = 48, out_channels = 64, kernel_size = (5, 5), padding = 'same')\n",
    "        self.conv_2_b = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = (5, 5), padding = 'same')\n",
    "        self.batch_norm_2 = nn.BatchNorm2d(64)\n",
    "        self.pool_2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.resize_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 64, out_channels = 96, kernel_size = (2, 2), stride = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(96)\n",
    "        )\n",
    "        \n",
    "        self.conv_3_a = nn.Conv2d(in_channels = 64, out_channels = 96, kernel_size = (5, 5), padding = 'same')\n",
    "        self.conv_3_b = nn.Conv2d(in_channels = 96, out_channels = 96, kernel_size = (5, 5), padding = 'same')\n",
    "        self.batch_norm_3 = nn.BatchNorm2d(96)\n",
    "        self.pool_3 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv_4_a = nn.Conv2d(in_channels = 96, out_channels = 128, kernel_size = (5, 5), padding = 'same')\n",
    "        self.conv_4_b = nn.Conv2d(in_channels = 128, out_channels = 96, kernel_size = (5, 5), padding = 'same')\n",
    "        self.batch_norm_4 = nn.BatchNorm2d(96)\n",
    "        self.pool_4 = nn.MaxPool2d(kernel_size = (7, 7), stride = (3, 3))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_1_a(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv_1_b(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batch_norm_1(x)\n",
    "        x = self.pool_1(x)\n",
    "        \n",
    "        x_resize_1 = self.resize_1(x)\n",
    "        \n",
    "        x = self.conv_2_a(x)\n",
    "        x = self.relu(x)      \n",
    "        x = self.conv_2_b(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batch_norm_2(x)\n",
    "        x = self.pool_2(x)\n",
    "        \n",
    "        x_conv_2 = x\n",
    "        x_resize_2 = self.resize_2(x_conv_2)\n",
    "\n",
    "        x = torch.add(x, x_resize_1)\n",
    "        \n",
    "        x = self.conv_3_a(x)\n",
    "        x = self.relu(x)      \n",
    "        x = self.conv_3_b(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batch_norm_3(x)\n",
    "        x = self.pool_3(x)\n",
    "        x = torch.add(x, x_resize_2)\n",
    "        \n",
    "        x = self.conv_4_a(x)\n",
    "        x = self.relu(x)      \n",
    "        x = self.conv_4_b(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batch_norm_4(x)\n",
    "        x = self.pool_4(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class BrinTumor(nn.Module):\n",
    "    def __init__(self, micro, macro, meso, num_in, num_out):\n",
    "        super(BrinTumor, self).__init__()\n",
    "        self.micro_layer = self._make_layer(micro, num_in)\n",
    "        self.macro_layer = self._make_layer(macro, num_in)\n",
    "        self.meso_layer = self._make_layer(meso, num_in)\n",
    "        \n",
    "        self.flatten = nn.Flatten(1)\n",
    "        \n",
    "        self.fc_1 = nn.Linear(in_features = 288 * 8 * 8, out_features = 4096)\n",
    "        self.drop_out = nn.Dropout(0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc_2 = nn.Linear(in_features = 4096, out_features = 512)\n",
    "        self.out_layer = nn.Linear(in_features = 512, out_features = num_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_macro = self.macro_layer(x)\n",
    "        x_meso = self.meso_layer(x)\n",
    "        x_micro = self.micro_layer(x)\n",
    "        \n",
    "        x = torch.cat((x_micro, x_meso, x_macro), 1)\n",
    "        \n",
    "        # print(x.shape)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.drop_out(x)\n",
    "\n",
    "        x = self.fc_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop_out(x)\n",
    "        x = self.fc_2(x)\n",
    "        x = self.relu(x)\n",
    "        out = self.out_layer(x)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def _make_layer(self, Module, num_in):\n",
    "        # layers = []\n",
    "        \n",
    "        self.in_channels = num_in\n",
    "        # layers.append(Module(self.in_channels))\n",
    "        layers = Module(self.in_channels)\n",
    "                    \n",
    "        return nn.Sequential(layers)\n",
    "    \n",
    "def BrinTumor_Main(num_in, num_out):\n",
    "    return BrinTumor(MICRO_SEM, MACRO_SEM, MESO_SEM, num_in, num_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class SegmentationTransform:\n",
    "    def __init__(self):\n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                 [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "            \n",
    "    def __call__(self, image, mask):\n",
    "        image = self.image_transform(image)\n",
    "        mask = mask.resize((224, 224), resample=Image.NEAREST)\n",
    "        mask = np.array(mask, dtype=np.float32)\n",
    "        mask = np.expand_dims(mask, axis=0)  # Add channel dimension\n",
    "        mask = torch.from_numpy(mask)\n",
    "        mask = mask / 255.0  # Scale mask to [0, 1] range if needed\n",
    "        return image, mask\n",
    "\n",
    "train_dataset = ClassificationDataset(root_dir='./Dataset_Arghadip/Train', transform=classification_transform)\n",
    "val_dataset = ClassificationDataset(root_dir='./Dataset_Arghadip/Validation', transform=classification_transform)\n",
    "test_dataset = ClassificationDataset(root_dir='./Dataset_Arghadip/Test', transform=classification_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "segmentation_image_dir = './Segmentaion_dataset/images'\n",
    "segmentation_mask_dir = './Segmentaion_dataset/masks'\n",
    "\n",
    "segmentation_transform = SegmentationTransform()\n",
    "segmentation_dataset = SegmentationDataset(image_dir=segmentation_image_dir,\n",
    "                                           mask_dir=segmentation_mask_dir,\n",
    "                                           transform=segmentation_transform)\n",
    "\n",
    "segmentation_loader = DataLoader(segmentation_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification_model = models.resnet18(pretrained=True)\n",
    "# num_ftrs = classification_model.fc.in_features\n",
    "# classification_model.fc = nn.Linear(num_ftrs, 4)  # 4 classes\n",
    "# classification_model = classification_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segmentation_model = models.segmentation.fcn_resnet50(pretrained=False, num_classes=1)\n",
    "# segmentation_model = segmentation_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1 = MICRO_SEM(3)\n",
    "# model2 = MESO_SEM(3)\n",
    "# model3 = MACRO_SEM(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchinfo import summary\n",
    "# summary_ = summary(model3, input_size = (32, 3, 224, 224))\n",
    "# summary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = BrinTumor_Main(3, 4)\n",
    "classification_model = classification_model.to(device)\n",
    "segmentation_model = models.segmentation.fcn_resnet50(pretrained = False, num_classes=1)\n",
    "segmentation_model = segmentation_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "summary_ = summary(classification_model, input_size = (32, 3, 224, 224))\n",
    "summary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_classification = nn.CrossEntropyLoss()\n",
    "optimizer_classification = optim.Adam(classification_model.parameters(), lr=1e-4)\n",
    "\n",
    "criterion_segmentation = nn.BCEWithLogitsLoss()\n",
    "optimizer_segmentation = optim.Adam(segmentation_model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 10\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     running_corrects = 0\n",
    "    \n",
    "#     for inputs, labels in train_loader:\n",
    "#         inputs = inputs.to(device)\n",
    "#         labels = labels.to(device)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         outputs = model(inputs)\n",
    "#         _, preds = torch.max(outputs, 1)\n",
    "#         loss = criterion(outputs, labels)\n",
    "        \n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         running_loss += loss.item() * inputs.size(0)\n",
    "#         running_corrects += torch.sum(preds == labels.data)\n",
    "    \n",
    "#     epoch_loss = running_loss / len(train_loader)\n",
    "#     epoch_acc = running_corrects.double() / len(train_loader)\n",
    "    \n",
    "#     print('Epoch {}/{}: Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "#         epoch+1, num_epochs, epoch_loss, epoch_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "num_epochs = 100\n",
    "total_len = len(train_loader) + len(val_loader)\n",
    "\n",
    "train_losses = np.zeros(num_epochs)\n",
    "test_losses = np.zeros(num_epochs)\n",
    "train_acc = np.zeros(num_epochs)\n",
    "test_acc = np.zeros(num_epochs)\n",
    "\n",
    "best_test_acc = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_str = str(epoch + 1).rjust(len(str(num_epochs)), \" \")\n",
    "    with tqdm(total = total_len, desc = f\"Epoch [ {epoch_str}/{num_epochs} ] : \") as pbar:\n",
    "        \n",
    "        classification_model.train()\n",
    "        n_correct = 0\n",
    "        n_total = 0\n",
    "        train_loss = []\n",
    "        \n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer_classification.zero_grad()\n",
    "            \n",
    "            outputs = classification_model(inputs)\n",
    "            loss = criterion_classification(outputs, targets)\n",
    "            train_loss.append(loss.item())\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer_classification.step()\n",
    "            \n",
    "            _, prediction = torch.max(outputs, 1)\n",
    "            \n",
    "            n_correct += (prediction == targets).sum().item()\n",
    "            n_total += targets.shape[0]\n",
    "            \n",
    "            pbar.update(1)\n",
    "        \n",
    "        # before_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        # scheduler.step()\n",
    "        # after_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        \n",
    "        train_loss = np.mean(train_loss)\n",
    "        train_acc_real = n_correct/n_total\n",
    "        train_losses[epoch] = train_loss\n",
    "        train_acc[epoch] = train_acc_real\n",
    "                \n",
    "        classification_model.eval()\n",
    "        n_correct = 0\n",
    "        n_total = 0\n",
    "        test_loss = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                \n",
    "                outputs = classification_model(inputs)\n",
    "                \n",
    "                loss = criterion_classification(outputs, targets)\n",
    "                test_loss.append(loss.item())\n",
    "                \n",
    "                _, prediction = torch.max(outputs, 1)\n",
    "                \n",
    "                n_correct += (prediction == targets).sum().item()\n",
    "                n_total += targets.shape[0]\n",
    "                \n",
    "                pbar.update(1)\n",
    "            \n",
    "        test_loss = np.mean(test_loss)\n",
    "        test_acc_real = n_correct/n_total\n",
    "        test_losses[epoch] = test_loss\n",
    "        test_acc[epoch] = test_acc_real\n",
    "        \n",
    "        pbar.set_description(f\"Epoch [ {epoch_str}/{num_epochs} ] \")\n",
    "        pbar.set_postfix({'Train acc' : f'{train_acc_real:.3f}',\n",
    "                          'Train loss' : f'{train_loss:.3f}',\n",
    "                          'Test acc' : f'{test_acc_real:.3f}',\n",
    "                          'Test loss': f'{test_loss:.3f}'\n",
    "                        #   'lr_before' : f'{before_lr}',\n",
    "                        #   'lr_after' : f'{after_lr}'+\n",
    "                         })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model.eval()\n",
    "val_running_corrects = 0\n",
    "\n",
    "n_correct = 0\n",
    "n_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "                \n",
    "        outputs = classification_model(inputs)\n",
    "\n",
    "        _, prediction = torch.max(outputs, 1)\n",
    "                \n",
    "        n_correct += (prediction == targets).sum().item()\n",
    "        n_total += targets.shape[0]\n",
    "\n",
    "test_acc_real = n_correct/n_total\n",
    "print('Validation Accuracy: {:.4f}'.format(test_acc_real))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm as tqdm_sim\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    segmentation_model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, masks) in tqdm_sim.tqdm(enumerate(segmentation_loader), total = len(segmentation_loader)):\n",
    "        inputs = inputs.to(device)\n",
    "        masks = masks.to(device)\n",
    "        optimizer_segmentation.zero_grad()\n",
    "        outputs = segmentation_model(inputs)['out']\n",
    "        loss = criterion_segmentation(outputs, masks.float())\n",
    "        loss.backward()\n",
    "        optimizer_segmentation.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "    epoch_loss = running_loss / len(segmentation_dataset)\n",
    "    print('Segmentation Epoch {}/{} Loss: {:.4f}'.format(epoch+1, num_epochs, epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "classification_model.eval()\n",
    "segmentation_model.eval()\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "classification_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "segmentation_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_image_path = './Dataset_Arghadip/Test/glioma/glioma (1).jpg'\n",
    "original_image = Image.open(test_image_path).convert('RGB')\n",
    "\n",
    "input_image_classification = classification_transform(original_image)\n",
    "input_image_classification = input_image_classification.unsqueeze(0).to(device)  # Shape: [1, 3, 224, 224]\n",
    "\n",
    "input_image_segmentation = segmentation_transform(original_image)\n",
    "input_image_segmentation = input_image_segmentation.unsqueeze(0).to(device)  # Shape: [1, 3, 224, 224]\n",
    "\n",
    "classification_output = classification_model(input_image_classification)\n",
    "_, predicted_class_idx = torch.max(classification_output, 1)\n",
    "predicted_class_idx = predicted_class_idx.item()\n",
    "\n",
    "idx_to_class = {0: 'glioma', 1: 'meningioma', 2: 'pituitary', 3: 'notumor'}\n",
    "predicted_class_name = idx_to_class[predicted_class_idx]\n",
    "\n",
    "segmentation_output = segmentation_model(input_image_segmentation)['out']  # Output shape: [1, 1, 224, 224]\n",
    "\n",
    "segmentation_output = segmentation_output.squeeze(0).cpu()  # Shape: [1, 224, 224]\n",
    "segmentation_output = torch.sigmoid(segmentation_output)  # Values between 0 and 1\n",
    "threshold = 0.5\n",
    "mask = (segmentation_output > threshold).float()\n",
    "mask = mask.squeeze().numpy()  # Shape: [224, 224]\n",
    "\n",
    "mask = Image.fromarray((mask * 255).astype(np.uint8))\n",
    "mask = mask.resize(original_image.size, resample=Image.NEAREST)\n",
    "\n",
    "overlay = original_image.copy()\n",
    "overlay.putalpha(mask)\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "axs[0].imshow(original_image)\n",
    "axs[0].set_title('Original Image')\n",
    "axs[0].axis('off')\n",
    "\n",
    "axs[1].imshow(mask, cmap='gray')\n",
    "axs[1].set_title('Segmentation Mask')\n",
    "axs[1].axis('off')\n",
    "\n",
    "axs[2].imshow(overlay)\n",
    "axs[2].set_title('Overlay')\n",
    "axs[2].axis('off')\n",
    "\n",
    "axs[3].text(0.5, 0.5, f'Predicted Class:\\n{predicted_class_name}', fontsize=18, ha='center')\n",
    "axs[3].set_axis_off()\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
